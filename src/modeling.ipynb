{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For visualization\n",
    "import altair as alt\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Models\n",
    "from sklearn import tree \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train = pd.read_csv(\"../data/processed_data/home_train.csv\") \n",
    "X = train.drop(columns = ['CLAIM', 'Unnamed: 0'])\n",
    "y = train['CLAIM']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (74009, 83)\n",
      "X valid shape: (18503, 83)\n",
      "y train shape: (74009,)\n",
      "y valid shape: (18503,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"X train shape: {X_train.shape}\")\n",
    "print(f\"X valid shape: {X_valid.shape}\")\n",
    "print(f\"y train shape: {y_train.shape}\")\n",
    "print(f\"y valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will select the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_report(model, X, y, Xv, yv, mode = 'regression'):\n",
    "    \"\"\"\n",
    "    Fits a given model and calculates its score in the training and the validation set.\n",
    "    ----------------------------------------------------\n",
    "    \n",
    "    Paramaters:\n",
    "    ----------------------------------------------------\n",
    "    model: model to fit\n",
    "    X: Training X matrix\n",
    "    y: Training response vector\n",
    "    Xv: Validation X matrix\n",
    "    yv: Validation response vector\n",
    "    mode: Type of estimation classification and\n",
    "    \n",
    "    Returns:\n",
    "    ---------------------------------------------------\n",
    "    errors: list with the training and validation error\n",
    "    \n",
    "    Example:\n",
    "    --------------------------------------------------\n",
    "    fit_and_report(LogisticRegression(), X, y, Xv, yv, mode = 'classification')\n",
    "    \"\"\"\n",
    "    model.fit(X, y)\n",
    "    if mode.lower().startswith('regress'):\n",
    "        errors = [mean_squared_error(y, model.predict(X)), mean_squared_error(yv, model.predict(Xv))]\n",
    "    if mode.lower().startswith('classif'):\n",
    "        errors = [1 - model.score(X,y), 1 - model.score(Xv,yv)]        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n"
     ]
    }
   ],
   "source": [
    "# This section is using the RFE function to select the most important variables\n",
    "# I chose 40 variables to test but I am interested in the order and the error\n",
    "n_features = 40\n",
    "\n",
    "# This dictionary stores the results\n",
    "results = {'Stage':[],\n",
    "           'N_features':[],\n",
    "           'Score':[]}\n",
    "\n",
    "for i in range(1, n_features + 1):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    print(f\"iteration {i}\")\n",
    "    # I am using a logistic regression as a \n",
    "    lr = LogisticRegression(solver = \"liblinear\", class_weight='balanced')\n",
    "    rfe = RFE(estimator = lr, n_features_to_select = i)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    \n",
    "    selected_features = rfe.support_\n",
    "    print(f\"Selected Features: {X_train.columns[selected_features]}\")\n",
    "    \n",
    "    \n",
    "    scores = fit_and_report(lr, \n",
    "                            X_train.iloc[:,selected_features], \n",
    "                            y_train, \n",
    "                            X_valid.iloc[:,selected_features], \n",
    "                            y_valid,\n",
    "                            mode=\"classification\")\n",
    "    \n",
    "    end = time.time() - start\n",
    "    print(f\"Time: {end}\")\n",
    "    print(\"-------------------\")\n",
    "    results['Stage'].append('Train')\n",
    "    results['N_features'].append(i)\n",
    "    results['Score'].append(scores[0])\n",
    "    results['Stage'].append('Validation')\n",
    "    results['N_features'].append(i)\n",
    "    results['Score'].append(scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_selection = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section shows the results of the variable selection process\n",
    "\n",
    "alt.Chart(variable_selection).mark_line().encode(\n",
    "    alt.Y('Score:Q'),\n",
    "    alt.X('N_features:O', title = 'Number of features'),\n",
    "    alt.Color('Stage:N')\n",
    ").properties(\n",
    "    width = 600,\n",
    "    height = 400,\n",
    "    title = 'Score by number of features using RFE'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_selection.to_csv(\"../results/data/variable_selection.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given it is a classification model, I am going to fit a:\n",
    "- Decision Tree\n",
    "- KNN classifier\n",
    "- Logistic regression\n",
    "- Support vector classifier\n",
    "- Random forest\n",
    "- XGboost\n",
    "- lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen variables\n",
    "variables = ['NCD_GRANTED_YEARS_C', 'AD_CONTENTS_Y', 'PAYMENT_METHOD_NonDD', 'PAYMENT_METHOD_PureDD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def evaluate_model(X_train, y_train, X_valid, y_valid, models):\n",
    "    \"\"\"\n",
    "    Evaluates a group of models\n",
    "\n",
    "    Parameters:\n",
    "    X_valid -- (dataframe) validation X\n",
    "    X_train -- (dataframe) train X\n",
    "    y_valid -- (series) validation y\n",
    "    y_train -- (series) train y\n",
    "    models -- (dictionary) models dictionary\n",
    "    \n",
    "    Returns:\n",
    "    results -- (dictionary) dictionary containing model, train error, validation error\n",
    "    and elapsed training and validation time\n",
    "    \n",
    "    importances -- (dataframe) feature importances\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "        \n",
    "    for model_name, model in models.items():\n",
    "        \n",
    "        # Timing the model\n",
    "        print(f\"Fitting model: {model_name}\")\n",
    "        t = time.time()\n",
    "\n",
    "        # Fitting the model as a pipeline\n",
    "\n",
    "        model.fit(X_train, y_train);\n",
    "        tr_err, valid_err = model.score(X_train, y_train), model.score(X_valid, y_valid)\n",
    "        \n",
    "        elapsed_time = time.time() - t\n",
    "        \n",
    "        results[model_name] = [model, round(tr_err,3), round(valid_err,3), round(elapsed_time,4)]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models to use\n",
    "models = {\n",
    "          'decision tree': DecisionTreeClassifier(class_weight='balanced'),\n",
    "          'kNN': KNeighborsClassifier(),\n",
    "          'logistic regression': LogisticRegression(solver ='liblinear', class_weight='balanced'),\n",
    "          'RBF SVM' :  SVC(gamma = 'scale', class_weight='balanced'), \n",
    "          'random forest' : RandomForestClassifier(class_weight='balanced'), \n",
    "          'xgboost' : XGBClassifier(),\n",
    "          'lgbm': LGBMClassifier(class_weight='balanced'),\n",
    "          'Dummy': DummyClassifier(strategy='stratified')\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model: decision tree\n",
      "Fitting model: kNN\n",
      "Fitting model: logistic regression\n",
      "Fitting model: RBF SVM\n",
      "Fitting model: random forest\n",
      "Fitting model: xgboost\n",
      "Fitting model: lgbm\n",
      "Fitting model: Dummy\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_model(X_train.loc[:,variables], y_train, X_valid.loc[:,variables], y_valid, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: decision tree\n",
      "Confusion matrix: [[62483  2827]\n",
      " [ 1525  7174]]\n",
      "The recall is 0.835\n",
      "The precision is 0.724\n",
      "The f1 is 0.775\n",
      "--------\n",
      "Model Name: kNN\n",
      "Confusion matrix: [[64717   593]\n",
      " [ 2311  6388]]\n",
      "The recall is 0.745\n",
      "The precision is 0.918\n",
      "The f1 is 0.822\n",
      "--------\n",
      "Model Name: logistic regression\n",
      "Confusion matrix: [[60116  5194]\n",
      " [ 1376  7323]]\n",
      "The recall is 0.857\n",
      "The precision is 0.594\n",
      "The f1 is 0.701\n",
      "--------\n",
      "Model Name: RBF SVM\n",
      "Confusion matrix: [[62483  2827]\n",
      " [ 1525  7174]]\n",
      "The recall is 0.835\n",
      "The precision is 0.724\n",
      "The f1 is 0.775\n",
      "--------\n",
      "Model Name: random forest\n",
      "Confusion matrix: [[62483  2827]\n",
      " [ 1525  7174]]\n",
      "The recall is 0.835\n",
      "The precision is 0.724\n",
      "The f1 is 0.775\n",
      "--------\n",
      "Model Name: xgboost\n",
      "Confusion matrix: [[64641   669]\n",
      " [ 2228  6471]]\n",
      "The recall is 0.754\n",
      "The precision is 0.910\n",
      "The f1 is 0.825\n",
      "--------\n",
      "Model Name: lgbm\n",
      "Confusion matrix: [[62483  2827]\n",
      " [ 1525  7174]]\n",
      "The recall is 0.835\n",
      "The precision is 0.724\n",
      "The f1 is 0.775\n",
      "--------\n",
      "Model Name: Dummy\n",
      "Confusion matrix: [[57674  7636]\n",
      " [ 7669  1030]]\n",
      "The recall is 0.117\n",
      "The precision is 0.117\n",
      "The f1 is 0.117\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in results.items():\n",
    "    print(f\"Model Name: {model_name}\")\n",
    "    cm = confusion_matrix(y_valid, model[0].predict(X_valid[variables]))\n",
    "    cm_train = confusion_matrix(y_train, model[0].predict(X_train[variables]))\n",
    "    print(f\"Confusion matrix: {cm_train}\")\n",
    "\n",
    "    tp = cm[1,1]\n",
    "    fn = cm[1,0]\n",
    "    fp = cm[0,1]\n",
    "    \n",
    "    recall = tp/(tp+fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    f1 = 2 * precision * recall/(precision + recall)\n",
    "    \n",
    "    results[model_name].append(recall)\n",
    "    results[model_name].append(precision)\n",
    "    results[model_name].append(f1)\n",
    "    \n",
    "    print(f\"The recall is {recall:.3f}\")\n",
    "    print(f\"The precision is {precision:.3f}\")\n",
    "    print(f\"The f1 is {f1:.3f}\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model\n",
    "fm = results['xgboost'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, index=['model', 'Train_score', 'Test_score', 'Train_test_time', 'Recall', 'Precision', 'F1']).T\n",
    "results_df.to_csv(\"../results/data/results_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the model\n",
    "filename = '../results/final_model.sav'\n",
    "pickle.dump(fm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = pd.read_csv(\"../data/processed_data/home_test.csv\") \n",
    "X_test_final = test_final.loc[:,variables]\n",
    "y_test_final = test_final['CLAIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = fm.predict(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_final, y_test_predict)\n",
    "\n",
    "tp = cm[1,1]\n",
    "fn = cm[1,0]\n",
    "fp = cm[0,1]\n",
    "tn = cm[0,0]\n",
    "    \n",
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "f1 = 2 * precision * recall/(precision + recall)\n",
    "\n",
    "score = fm.score(X_test_final,y_test_final)\n",
    "\n",
    "print(f\"The recall is {recall:.3f}\")\n",
    "print(f\"The precision is {precision:.3f}\")\n",
    "print(f\"The f1 is {f1:.3f}\")\n",
    "print(f\"The score {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = {\n",
    "    'tp':tp,\n",
    "    'fp':fp,\n",
    "    'tn':tn,\n",
    "    'fn':fn,\n",
    "    'recall':recall,\n",
    "    'precision':precision,\n",
    "    'f1': f1,\n",
    "    'score':score\n",
    "}\n",
    "\n",
    "results_test_df = pd.DataFrame(results_test, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test_df.to_csv(\"../results/data/results_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
